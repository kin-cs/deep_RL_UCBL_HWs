{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Code to load an expert policy and generate roll-out data for behavioral cloning.\n",
    "Example usage:\n",
    "    python run_expert.py experts/Humanoid-v1.pkl Humanoid-v1 --render \\\n",
    "            --num_rollouts 20\n",
    "\n",
    "Author of this script and included expert policies: Jonathan Ho (hoj@openai.com)\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_util\n",
    "import gym\n",
    "import load_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('expert_policy_file', type=str)\n",
    "parser.add_argument('envname', type=str)\n",
    "parser.add_argument('--render', action='store_true')\n",
    "parser.add_argument(\"--max_timesteps\", type=int)\n",
    "parser.add_argument('--num_rollouts', type=int, default=20,\n",
    "                    help='Number of expert roll outs')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expert_policy_file = 'experts/Humanoid-v1.pkl'\n",
    "envname = 'Humanoid-v1'\n",
    "render = False\n",
    "num_rollouts = 100\n",
    "max_timesteps = 500 # (default=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 376) (1, 376)\n",
      "loaded and built\n"
     ]
    }
   ],
   "source": [
    "print('loading and building expert policy')\n",
    "policy_fn = load_policy.load_policy(expert_policy_file)\n",
    "print('loaded and built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Volumes/JetDrive/MOOC/UCBerkeley/Reinforcement Learning 2017 Fall/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 19:01:33,860] From /Volumes/JetDrive/MOOC/UCBerkeley/Reinforcement Learning 2017 Fall/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Volumes/JetDrive/MOOC/UCBerkeley/Reinforcement Learning 2017 Fall/homework/hw1/tf_util.py:92: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 19:01:33,861] From /Volumes/JetDrive/MOOC/UCBerkeley/Reinforcement Learning 2017 Fall/homework/hw1/tf_util.py:92: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n",
      "[2018-01-13 19:01:33,876] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 1\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 2\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 3\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 4\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 5\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 6\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 7\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 8\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 9\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 10\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 11\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 12\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 13\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 14\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 15\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 16\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 17\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 18\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 19\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 20\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 21\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 22\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 23\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 24\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 25\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 26\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 27\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 28\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 29\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 30\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 31\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 32\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 33\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 34\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 35\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 36\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 37\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 38\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 39\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 40\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 41\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 42\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 43\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 44\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 45\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 46\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 47\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 48\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 49\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 50\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 51\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 52\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 53\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 54\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 55\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 56\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 57\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 58\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 59\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 60\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 61\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 62\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 63\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 64\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 65\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 66\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 67\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 68\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 69\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 70\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 71\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 72\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 73\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 74\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 75\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 76\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 77\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 78\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 79\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 80\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 81\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 82\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 83\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 84\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 85\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 86\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 87\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 88\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 89\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 90\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 91\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 92\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 93\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 94\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 95\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 96\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 97\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 98\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 99\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "returns [4994.8630536518067, 4932.9506936581802, 4908.5246427524871, 4879.2461128797622, 4939.0140704493797, 4911.2904198483157, 4921.266245090118, 4920.2270699414348, 4940.3877852297137, 4955.3482642902727, 4963.8432423079948, 4787.3344730088211, 4972.8773467886331, 4888.6374704151385, 4891.8368144691549, 4969.7401708541192, 4974.4243788560925, 4869.8691471202856, 4948.4238714182538, 4866.3901678904986, 4934.15005010611, 4913.5016586127012, 4887.5517799427134, 4913.2920218363643, 4886.1307542705117, 4869.1390630059641, 4929.6332973012404, 4922.7233232573499, 4919.2245928702168, 4970.1759721772842, 4971.4599897884091, 4853.4225044216455, 4858.6159547302186, 4820.9308328050211, 4861.4599510431644, 4947.2962242311605, 4877.1979650425128, 4863.1587185197614, 4942.2420279554826, 4932.2401553327763, 4944.7503765956299, 4970.7237741259578, 4933.4721460986166, 4924.9182946246292, 4905.5574138197999, 4931.2187384313729, 4878.1024771925513, 4885.9213493691295, 4872.0218367094703, 4959.1693484537009, 4956.3894499049775, 4900.8899620132288, 4933.1544407360188, 4915.7627765297675, 4941.8410280807584, 4916.1661633293625, 4862.3012070350069, 4887.5045835778401, 4968.9365322530348, 4949.1232652466479, 4956.4757832333944, 4906.7689455191457, 4853.3273331614437, 4904.2793070801208, 4960.3204430658407, 4954.3915413295099, 4860.6134558991889, 4909.9663874854887, 4922.9762556781188, 4831.745602210025, 4890.4631296066254, 4856.9123140924276, 4904.001168507245, 4831.9313945963886, 4890.8724679826055, 4950.6436029519928, 4910.217845858253, 4969.5916807421354, 4914.2664555575493, 4921.6515964152131, 4986.1463126738272, 4896.2072638692543, 4991.7831549432203, 4828.5728150578725, 4931.2956755763917, 4966.3688657930825, 4851.0923420120198, 4955.184077607265, 4900.8006157320951, 4908.5320899385706, 4898.1561791670501, 4923.4994058866196, 4875.8496211122565, 5000.2319979852873, 4916.1884429475094, 4977.289452784983, 4941.0838196583363, 4943.9503662433071, 4864.1871224023344, 4937.1074449602957]\n",
      "mean return 4915.4291122\n",
      "std of return 42.838058301\n"
     ]
    }
   ],
   "source": [
    "with tf.Session():\n",
    "    tf_util.initialize()\n",
    "\n",
    "    import gym\n",
    "    env = gym.make(envname)\n",
    "    max_steps = max_timesteps # default is: env.spec.timestep_limit\n",
    "\n",
    "    returns = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    for i in range(num_rollouts):\n",
    "        print('iter', i)\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        totalr = 0.\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = policy_fn(obs[None,:])\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            totalr += r\n",
    "            steps += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "            if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "        returns.append(totalr)\n",
    "\n",
    "    print('returns', returns)\n",
    "    print('mean return', np.mean(returns))\n",
    "    print('std of return', np.std(returns))\n",
    "\n",
    "    expert_data = {'observations': np.array(observations),\n",
    "                   'actions': np.array(actions)}\n",
    "    output_file_name = 'data/' + envname + '_' + str(num_rollouts) + '_data.pkl'\n",
    "    with open(output_file_name, 'wb') as f:\n",
    "        pickle.dump(expert_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result of reward for Expert Policy\n",
    "- mean return 4915.4291122\n",
    "- std of return 42.838058301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a NN to learn from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.loads(f.read())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data_path = 'data/Humanoid-v1_100_data.pkl'\n",
    "data = load_data(training_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers of data points\n",
    "n_data = len(data['observations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numbers of parameters in one observation\n",
    "len(data['observations'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_data = data['observations']\n",
    "act_data = data['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_val = int(n_data * .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = obs_data[:split_val]\n",
    "y_train = act_data[:split_val]\n",
    "x_val = obs_data[split_val:]\n",
    "y_val = act_data[split_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1, 17)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0], y_train.shape[2])\n",
    "y_val = y_val.reshape(y_val.shape[0], y_val.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 376)\n",
      "(40000, 17)\n",
      "(10000, 376)\n",
      "(10000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "loss_fuc = 'mean_squared_error' # or 'msle'\n",
    "\n",
    "# construct the callback\n",
    "filepath=\"data/best_epoch.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(act_data.shape[2], activation='linear'))\n",
    "model.compile(loss=loss_fuc, optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 256)               96512     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 17)                4369      \n",
      "=================================================================\n",
      "Total params: 232,465\n",
      "Trainable params: 232,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 2.6325Epoch 00000: val_loss improved from inf to 0.36354, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 2.6289 - val_loss: 0.3635\n",
      "Epoch 2/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.3407Epoch 00001: val_loss improved from 0.36354 to 0.27298, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 3s - loss: 0.3403 - val_loss: 0.2730\n",
      "Epoch 3/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.2829Epoch 00002: val_loss improved from 0.27298 to 0.16530, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.2816 - val_loss: 0.1653\n",
      "Epoch 4/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.1703Epoch 00003: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.1706 - val_loss: 0.2347\n",
      "Epoch 5/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.2987Epoch 00004: val_loss improved from 0.16530 to 0.12770, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.2976 - val_loss: 0.1277\n",
      "Epoch 6/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.1554Epoch 00005: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.1561 - val_loss: 0.1410\n",
      "Epoch 7/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.2099Epoch 00006: val_loss improved from 0.12770 to 0.11372, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.2089 - val_loss: 0.1137\n",
      "Epoch 8/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.2555Epoch 00007: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.2546 - val_loss: 0.1151\n",
      "Epoch 9/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.1666Epoch 00008: val_loss improved from 0.11372 to 0.10184, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.1660 - val_loss: 0.1018\n",
      "Epoch 10/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.1181Epoch 00009: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.1183 - val_loss: 0.1985\n",
      "Epoch 11/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.1289Epoch 00010: val_loss improved from 0.10184 to 0.10148, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.1287 - val_loss: 0.1015\n",
      "Epoch 12/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.1009Epoch 00011: val_loss improved from 0.10148 to 0.08200, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.1007 - val_loss: 0.0820\n",
      "Epoch 13/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0871Epoch 00012: val_loss improved from 0.08200 to 0.07663, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0869 - val_loss: 0.0766\n",
      "Epoch 14/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0791Epoch 00013: val_loss improved from 0.07663 to 0.07161, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0790 - val_loss: 0.0716\n",
      "Epoch 15/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0764Epoch 00014: val_loss improved from 0.07161 to 0.06809, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0764 - val_loss: 0.0681\n",
      "Epoch 16/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0713Epoch 00015: val_loss improved from 0.06809 to 0.06707, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0713 - val_loss: 0.0671\n",
      "Epoch 17/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0684Epoch 00016: val_loss improved from 0.06707 to 0.06418, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0684 - val_loss: 0.0642\n",
      "Epoch 18/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0689Epoch 00017: val_loss improved from 0.06418 to 0.06294, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0689 - val_loss: 0.0629\n",
      "Epoch 19/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0633Epoch 00018: val_loss improved from 0.06294 to 0.06082, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0633 - val_loss: 0.0608\n",
      "Epoch 20/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0601Epoch 00019: val_loss improved from 0.06082 to 0.05979, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0601 - val_loss: 0.0598\n",
      "Epoch 21/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0565Epoch 00020: val_loss improved from 0.05979 to 0.05520, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0565 - val_loss: 0.0552\n",
      "Epoch 22/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0523Epoch 00021: val_loss improved from 0.05520 to 0.05336, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 5s - loss: 0.0522 - val_loss: 0.0534\n",
      "Epoch 23/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0501Epoch 00022: val_loss improved from 0.05336 to 0.05110, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0501 - val_loss: 0.0511\n",
      "Epoch 24/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0504Epoch 00023: val_loss improved from 0.05110 to 0.04746, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0503 - val_loss: 0.0475\n",
      "Epoch 25/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0481Epoch 00024: val_loss improved from 0.04746 to 0.04648, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0481 - val_loss: 0.0465\n",
      "Epoch 26/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0458Epoch 00025: val_loss did not improve\n",
      "40000/40000 [==============================] - 5s - loss: 0.0458 - val_loss: 0.0543\n",
      "Epoch 27/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0453Epoch 00026: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0452 - val_loss: 0.0480\n",
      "Epoch 28/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0432Epoch 00027: val_loss improved from 0.04648 to 0.04404, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0432 - val_loss: 0.0440\n",
      "Epoch 29/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0413Epoch 00028: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0413 - val_loss: 0.0450\n",
      "Epoch 30/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0401Epoch 00029: val_loss improved from 0.04404 to 0.04261, saving model to data/best_epoch.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 4s - loss: 0.0401 - val_loss: 0.0426\n",
      "Epoch 31/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0398Epoch 00030: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0398 - val_loss: 0.0452\n",
      "Epoch 32/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0386Epoch 00031: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0386 - val_loss: 0.0435\n",
      "Epoch 33/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0381Epoch 00032: val_loss improved from 0.04261 to 0.04130, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0381 - val_loss: 0.0413\n",
      "Epoch 34/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0373Epoch 00033: val_loss improved from 0.04130 to 0.04086, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0373 - val_loss: 0.0409\n",
      "Epoch 35/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0368Epoch 00034: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0368 - val_loss: 0.0421\n",
      "Epoch 36/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0361Epoch 00035: val_loss improved from 0.04086 to 0.04071, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0362 - val_loss: 0.0407\n",
      "Epoch 37/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0354Epoch 00036: val_loss improved from 0.04071 to 0.03849, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0354 - val_loss: 0.0385\n",
      "Epoch 38/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0351Epoch 00037: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0351 - val_loss: 0.0390\n",
      "Epoch 39/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0348Epoch 00038: val_loss improved from 0.03849 to 0.03785, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0348 - val_loss: 0.0378\n",
      "Epoch 40/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0348Epoch 00039: val_loss improved from 0.03785 to 0.03741, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 3s - loss: 0.0348 - val_loss: 0.0374\n",
      "Epoch 41/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0326Epoch 00040: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0326 - val_loss: 0.0380\n",
      "Epoch 42/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0328Epoch 00041: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0328 - val_loss: 0.0396\n",
      "Epoch 43/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0327Epoch 00042: val_loss improved from 0.03741 to 0.03660, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0328 - val_loss: 0.0366\n",
      "Epoch 44/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0326Epoch 00043: val_loss improved from 0.03660 to 0.03545, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0326 - val_loss: 0.0354\n",
      "Epoch 45/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0309Epoch 00044: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0309 - val_loss: 0.0356\n",
      "Epoch 46/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0306Epoch 00045: val_loss improved from 0.03545 to 0.03427, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0306 - val_loss: 0.0343\n",
      "Epoch 47/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0306Epoch 00046: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0306 - val_loss: 0.0365\n",
      "Epoch 48/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0301Epoch 00047: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0301 - val_loss: 0.0369\n",
      "Epoch 49/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0299Epoch 00048: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0299 - val_loss: 0.0367\n",
      "Epoch 50/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0292Epoch 00049: val_loss improved from 0.03427 to 0.03403, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0291 - val_loss: 0.0340\n",
      "Epoch 51/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0293Epoch 00050: val_loss improved from 0.03403 to 0.03289, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0293 - val_loss: 0.0329\n",
      "Epoch 52/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0285Epoch 00051: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0286 - val_loss: 0.0357\n",
      "Epoch 53/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0322Epoch 00052: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0321 - val_loss: 0.0339\n",
      "Epoch 54/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0292Epoch 00053: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0291 - val_loss: 0.0347\n",
      "Epoch 55/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0282Epoch 00054: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0282 - val_loss: 0.0337\n",
      "Epoch 56/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0275Epoch 00055: val_loss improved from 0.03289 to 0.03269, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0275 - val_loss: 0.0327\n",
      "Epoch 57/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0274Epoch 00056: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0274 - val_loss: 0.0334\n",
      "Epoch 58/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0274Epoch 00057: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0274 - val_loss: 0.0335\n",
      "Epoch 59/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0270Epoch 00058: val_loss improved from 0.03269 to 0.03219, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0270 - val_loss: 0.0322\n",
      "Epoch 60/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0264Epoch 00059: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0264 - val_loss: 0.0325\n",
      "Epoch 61/200\n",
      "39360/40000 [============================>.] - ETA: 0s - loss: 0.0266Epoch 00060: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0266 - val_loss: 0.0327\n",
      "Epoch 62/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0265Epoch 00061: val_loss improved from 0.03219 to 0.03175, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 3s - loss: 0.0265 - val_loss: 0.0318\n",
      "Epoch 63/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0264Epoch 00062: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0265 - val_loss: 0.0329\n",
      "Epoch 64/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0285Epoch 00063: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0285 - val_loss: 0.0341\n",
      "Epoch 65/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0274Epoch 00064: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0274 - val_loss: 0.0322\n",
      "Epoch 66/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0287Epoch 00065: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 4s - loss: 0.0287 - val_loss: 0.0319\n",
      "Epoch 67/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0263Epoch 00066: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0263 - val_loss: 0.0322\n",
      "Epoch 68/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0266Epoch 00067: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0266 - val_loss: 0.0328\n",
      "Epoch 69/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0251Epoch 00068: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0251 - val_loss: 0.0326\n",
      "Epoch 70/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0248Epoch 00069: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0248 - val_loss: 0.0319\n",
      "Epoch 71/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0248Epoch 00070: val_loss improved from 0.03175 to 0.03170, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0248 - val_loss: 0.0317\n",
      "Epoch 72/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0247Epoch 00071: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0247 - val_loss: 0.0327\n",
      "Epoch 73/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0245Epoch 00072: val_loss improved from 0.03170 to 0.02997, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0245 - val_loss: 0.0300\n",
      "Epoch 74/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0245Epoch 00073: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0245 - val_loss: 0.0316\n",
      "Epoch 75/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0242Epoch 00074: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0242 - val_loss: 0.0306\n",
      "Epoch 76/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0247Epoch 00075: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0247 - val_loss: 0.0314\n",
      "Epoch 77/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0241Epoch 00076: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0241 - val_loss: 0.0303\n",
      "Epoch 78/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0241Epoch 00077: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0241 - val_loss: 0.0306\n",
      "Epoch 79/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0242Epoch 00078: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0242 - val_loss: 0.0314\n",
      "Epoch 80/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0235Epoch 00079: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0236 - val_loss: 0.0307\n",
      "Epoch 81/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0238Epoch 00080: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0238 - val_loss: 0.0303\n",
      "Epoch 82/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0235Epoch 00081: val_loss improved from 0.02997 to 0.02966, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0235 - val_loss: 0.0297\n",
      "Epoch 83/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0238Epoch 00082: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0238 - val_loss: 0.0323\n",
      "Epoch 84/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0276Epoch 00083: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0276 - val_loss: 0.0324\n",
      "Epoch 85/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0251Epoch 00084: val_loss did not improve\n",
      "40000/40000 [==============================] - 5s - loss: 0.0250 - val_loss: 0.0299\n",
      "Epoch 86/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0238Epoch 00085: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0238 - val_loss: 0.0304\n",
      "Epoch 87/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0257Epoch 00086: val_loss improved from 0.02966 to 0.02846, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0256 - val_loss: 0.0285\n",
      "Epoch 88/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0242Epoch 00087: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0242 - val_loss: 0.0303\n",
      "Epoch 89/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0227Epoch 00088: val_loss improved from 0.02846 to 0.02775, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0227 - val_loss: 0.0278\n",
      "Epoch 90/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0231Epoch 00089: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0231 - val_loss: 0.0310\n",
      "Epoch 91/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0227Epoch 00090: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0227 - val_loss: 0.0296\n",
      "Epoch 92/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0230Epoch 00091: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0230 - val_loss: 0.0286\n",
      "Epoch 93/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0225Epoch 00092: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0225 - val_loss: 0.0298\n",
      "Epoch 94/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0226Epoch 00093: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0225 - val_loss: 0.0290\n",
      "Epoch 95/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0221Epoch 00094: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0221 - val_loss: 0.0301\n",
      "Epoch 96/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0220Epoch 00095: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0220 - val_loss: 0.0279\n",
      "Epoch 97/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0220Epoch 00096: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0220 - val_loss: 0.0304\n",
      "Epoch 98/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0222Epoch 00097: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0222 - val_loss: 0.0295\n",
      "Epoch 99/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0227Epoch 00098: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0227 - val_loss: 0.0285\n",
      "Epoch 100/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0217Epoch 00099: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0217 - val_loss: 0.0296\n",
      "Epoch 101/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0220Epoch 00100: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0220 - val_loss: 0.0278\n",
      "Epoch 102/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0219Epoch 00101: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0219 - val_loss: 0.0296\n",
      "Epoch 103/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0233Epoch 00102: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0233 - val_loss: 0.0279\n",
      "Epoch 104/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0227Epoch 00103: val_loss improved from 0.02775 to 0.02768, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0227 - val_loss: 0.0277\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0213Epoch 00104: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0213 - val_loss: 0.0281\n",
      "Epoch 106/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0210Epoch 00105: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0210 - val_loss: 0.0281\n",
      "Epoch 107/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0215Epoch 00106: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0215 - val_loss: 0.0287\n",
      "Epoch 108/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0215Epoch 00107: val_loss improved from 0.02768 to 0.02669, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 3s - loss: 0.0214 - val_loss: 0.0267\n",
      "Epoch 109/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0215Epoch 00108: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0215 - val_loss: 0.0289\n",
      "Epoch 110/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0216Epoch 00109: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0216 - val_loss: 0.0293\n",
      "Epoch 111/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0212Epoch 00110: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0212 - val_loss: 0.0269\n",
      "Epoch 112/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0206Epoch 00111: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0206 - val_loss: 0.0288\n",
      "Epoch 113/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0208Epoch 00112: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0208 - val_loss: 0.0283\n",
      "Epoch 114/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0208Epoch 00113: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0209 - val_loss: 0.0270\n",
      "Epoch 115/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0234Epoch 00114: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0234 - val_loss: 0.0274\n",
      "Epoch 116/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0215Epoch 00115: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0215 - val_loss: 0.0278\n",
      "Epoch 117/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0211Epoch 00116: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0211 - val_loss: 0.0270\n",
      "Epoch 118/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0206Epoch 00117: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0206 - val_loss: 0.0271\n",
      "Epoch 119/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0207Epoch 00118: val_loss improved from 0.02669 to 0.02594, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0207 - val_loss: 0.0259\n",
      "Epoch 120/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0201Epoch 00119: val_loss improved from 0.02594 to 0.02586, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0201 - val_loss: 0.0259\n",
      "Epoch 121/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0204Epoch 00120: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0204 - val_loss: 0.0269\n",
      "Epoch 122/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0208Epoch 00121: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0208 - val_loss: 0.0287\n",
      "Epoch 123/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0205Epoch 00122: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0205 - val_loss: 0.0272\n",
      "Epoch 124/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0210Epoch 00123: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0210 - val_loss: 0.0269\n",
      "Epoch 125/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0200Epoch 00124: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0200 - val_loss: 0.0266\n",
      "Epoch 126/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0204Epoch 00125: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0204 - val_loss: 0.0296\n",
      "Epoch 127/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0209Epoch 00126: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0209 - val_loss: 0.0267\n",
      "Epoch 128/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0199Epoch 00127: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0199 - val_loss: 0.0279\n",
      "Epoch 129/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0200Epoch 00128: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0204 - val_loss: 0.0281\n",
      "Epoch 130/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0202Epoch 00129: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0202 - val_loss: 0.0259\n",
      "Epoch 131/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0201Epoch 00130: val_loss improved from 0.02586 to 0.02571, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0201 - val_loss: 0.0257\n",
      "Epoch 132/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0201Epoch 00131: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0201 - val_loss: 0.0264\n",
      "Epoch 133/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0195Epoch 00132: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0195 - val_loss: 0.0257\n",
      "Epoch 134/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0196Epoch 00133: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0196 - val_loss: 0.0259\n",
      "Epoch 135/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0200Epoch 00134: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0200 - val_loss: 0.0260\n",
      "Epoch 136/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0197Epoch 00135: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0197 - val_loss: 0.0272\n",
      "Epoch 137/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0197Epoch 00136: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0197 - val_loss: 0.0282\n",
      "Epoch 138/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0200Epoch 00137: val_loss improved from 0.02571 to 0.02547, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0200 - val_loss: 0.0255\n",
      "Epoch 139/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0195Epoch 00138: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0195 - val_loss: 0.0270\n",
      "Epoch 140/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0198Epoch 00139: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0198 - val_loss: 0.0281\n",
      "Epoch 141/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0202Epoch 00140: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0203 - val_loss: 0.0274\n",
      "Epoch 142/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0203Epoch 00141: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0203 - val_loss: 0.0262\n",
      "Epoch 143/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0195Epoch 00142: val_loss improved from 0.02547 to 0.02545, saving model to data/best_epoch.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 4s - loss: 0.0195 - val_loss: 0.0255\n",
      "Epoch 144/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0193Epoch 00143: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0193 - val_loss: 0.0258\n",
      "Epoch 145/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0191Epoch 00144: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0191 - val_loss: 0.0260\n",
      "Epoch 146/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0188Epoch 00145: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0188 - val_loss: 0.0258\n",
      "Epoch 147/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0198Epoch 00146: val_loss improved from 0.02545 to 0.02511, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0198 - val_loss: 0.0251\n",
      "Epoch 148/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0204Epoch 00147: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0204 - val_loss: 0.0268\n",
      "Epoch 149/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0194Epoch 00148: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0194 - val_loss: 0.0257\n",
      "Epoch 150/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0187Epoch 00149: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0187 - val_loss: 0.0257\n",
      "Epoch 151/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0189Epoch 00150: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0189 - val_loss: 0.0265\n",
      "Epoch 152/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0190Epoch 00151: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0190 - val_loss: 0.0262\n",
      "Epoch 153/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0192Epoch 00152: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0192 - val_loss: 0.0264\n",
      "Epoch 154/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0189Epoch 00153: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0189 - val_loss: 0.0270\n",
      "Epoch 155/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0310Epoch 00154: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0310 - val_loss: 0.0262\n",
      "Epoch 156/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0259Epoch 00155: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0259 - val_loss: 0.0257\n",
      "Epoch 157/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0212Epoch 00156: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0213 - val_loss: 0.0289\n",
      "Epoch 158/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0198Epoch 00157: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0198 - val_loss: 0.0257\n",
      "Epoch 159/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0195Epoch 00158: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0195 - val_loss: 0.0252\n",
      "Epoch 160/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0193Epoch 00159: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0193 - val_loss: 0.0262\n",
      "Epoch 161/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0199Epoch 00160: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0199 - val_loss: 0.0255\n",
      "Epoch 162/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0196Epoch 00161: val_loss improved from 0.02511 to 0.02510, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0196 - val_loss: 0.0251\n",
      "Epoch 163/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0188Epoch 00162: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0188 - val_loss: 0.0253\n",
      "Epoch 164/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0190Epoch 00163: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0190 - val_loss: 0.0257\n",
      "Epoch 165/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0201Epoch 00164: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0201 - val_loss: 0.0251\n",
      "Epoch 166/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0186Epoch 00165: val_loss improved from 0.02510 to 0.02480, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0186 - val_loss: 0.0248\n",
      "Epoch 167/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0187Epoch 00166: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0187 - val_loss: 0.0257\n",
      "Epoch 168/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0189Epoch 00167: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0189 - val_loss: 0.0266\n",
      "Epoch 169/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0185Epoch 00168: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0185 - val_loss: 0.0254\n",
      "Epoch 170/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0185Epoch 00169: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0185 - val_loss: 0.0252\n",
      "Epoch 171/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0186Epoch 00170: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0186 - val_loss: 0.0280\n",
      "Epoch 172/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0182Epoch 00171: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0182 - val_loss: 0.0248\n",
      "Epoch 173/200\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.0183Epoch 00172: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0183 - val_loss: 0.0254\n",
      "Epoch 174/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0180Epoch 00173: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0180 - val_loss: 0.0257\n",
      "Epoch 175/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0183Epoch 00174: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0183 - val_loss: 0.0254\n",
      "Epoch 176/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0182Epoch 00175: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0182 - val_loss: 0.0253\n",
      "Epoch 177/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0190Epoch 00176: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0190 - val_loss: 0.0266\n",
      "Epoch 178/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0183Epoch 00177: val_loss improved from 0.02480 to 0.02442, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 3s - loss: 0.0183 - val_loss: 0.0244\n",
      "Epoch 179/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0186Epoch 00178: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0186 - val_loss: 0.0259\n",
      "Epoch 180/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0191Epoch 00179: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0191 - val_loss: 0.0256\n",
      "Epoch 181/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0188Epoch 00180: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0189 - val_loss: 0.0263\n",
      "Epoch 182/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0202Epoch 00181: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 4s - loss: 0.0202 - val_loss: 0.0256\n",
      "Epoch 183/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0184Epoch 00182: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0184 - val_loss: 0.0248\n",
      "Epoch 184/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0187Epoch 00183: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0187 - val_loss: 0.0271\n",
      "Epoch 185/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0183Epoch 00184: val_loss improved from 0.02442 to 0.02343, saving model to data/best_epoch.hdf5\n",
      "40000/40000 [==============================] - 4s - loss: 0.0183 - val_loss: 0.0234\n",
      "Epoch 186/200\n",
      "39616/40000 [============================>.] - ETA: 0s - loss: 0.0182Epoch 00185: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0182 - val_loss: 0.0247\n",
      "Epoch 187/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0179Epoch 00186: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0179 - val_loss: 0.0239\n",
      "Epoch 188/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0182Epoch 00187: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0182 - val_loss: 0.0249\n",
      "Epoch 189/200\n",
      "39424/40000 [============================>.] - ETA: 0s - loss: 0.0183Epoch 00188: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0183 - val_loss: 0.0256\n",
      "Epoch 190/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0186Epoch 00189: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0186 - val_loss: 0.0256\n",
      "Epoch 191/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0189Epoch 00190: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0189 - val_loss: 0.0259\n",
      "Epoch 192/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0186Epoch 00191: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0186 - val_loss: 0.0241\n",
      "Epoch 193/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0196Epoch 00192: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0196 - val_loss: 0.0258\n",
      "Epoch 194/200\n",
      "39680/40000 [============================>.] - ETA: 0s - loss: 0.0204Epoch 00193: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0204 - val_loss: 0.0245\n",
      "Epoch 195/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0179Epoch 00194: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0179 - val_loss: 0.0240\n",
      "Epoch 196/200\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.0181Epoch 00195: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0181 - val_loss: 0.0246\n",
      "Epoch 197/200\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.0178Epoch 00196: val_loss did not improve\n",
      "40000/40000 [==============================] - 3s - loss: 0.0178 - val_loss: 0.0243\n",
      "Epoch 198/200\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.0179Epoch 00197: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0179 - val_loss: 0.0238\n",
      "Epoch 199/200\n",
      "39744/40000 [============================>.] - ETA: 0s - loss: 0.0179Epoch 00198: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0179 - val_loss: 0.0251\n",
      "Epoch 200/200\n",
      "39552/40000 [============================>.] - ETA: 0s - loss: 0.0178Epoch 00199: val_loss did not improve\n",
      "40000/40000 [==============================] - 4s - loss: 0.0179 - val_loss: 0.0241\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-59dd2bc48d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhistory_w_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_w_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "## Start Training\n",
    "model.summary()\n",
    "history_w_model = model.fit(x_train, y_train, callbacks=callbacks_list, epochs=num_epochs, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "plt.plot(history_w_model.history['loss'], label='loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.plot(history_w_model.history['val_loss'], label='Val_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzCY5FMzMbJJDwczMJjkUzMxs0v8HG/mtYRTbyH8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x146a67da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_w_model.history['loss'], label='loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.plot(history_w_model.history['val_loss'], label='Val_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load weights into the model\n",
    "model.load_weights(\"data/best_epoch.hdf5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 1\n",
      "100/500\n",
      "iter 2\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 3\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 4\n",
      "100/500\n",
      "200/500\n",
      "iter 5\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 6\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 7\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 8\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 9\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 10\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 11\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 12\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 13\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 14\n",
      "100/500\n",
      "200/500\n",
      "iter 15\n",
      "iter 16\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 17\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 18\n",
      "100/500\n",
      "200/500\n",
      "iter 19\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 20\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 21\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 22\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 23\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 24\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 25\n",
      "100/500\n",
      "iter 26\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 27\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 28\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 29\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 30\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 31\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 32\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 33\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 34\n",
      "100/500\n",
      "iter 35\n",
      "100/500\n",
      "iter 36\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 37\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 38\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 39\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 40\n",
      "100/500\n",
      "iter 41\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 42\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 43\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 44\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 45\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 46\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 47\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 48\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 49\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 50\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 51\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 52\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 53\n",
      "iter 54\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 55\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 56\n",
      "100/500\n",
      "iter 57\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 58\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 59\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 60\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 61\n",
      "100/500\n",
      "200/500\n",
      "iter 62\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 63\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 64\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 65\n",
      "100/500\n",
      "200/500\n",
      "iter 66\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 67\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 68\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 69\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 70\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 71\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 72\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 73\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 74\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 75\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 76\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 77\n",
      "100/500\n",
      "iter 78\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 79\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 80\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 81\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 82\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 83\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 84\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 85\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 86\n",
      "100/500\n",
      "200/500\n",
      "iter 87\n",
      "100/500\n",
      "iter 88\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 89\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 90\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 91\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 92\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 93\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 94\n",
      "100/500\n",
      "iter 95\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 96\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 97\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 98\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 99\n",
      "100/500\n",
      "returns [4829.8197062058161, 1464.6912046397815, 4915.9308481063172, 2820.5981079437624, 1687.0515135077285, 3586.7147413103989, 4959.3672150555567, 4847.3542884786884, 4887.5871245309045, 4933.5830438648727, 4802.084707870953, 3772.3346751510221, 4921.6189032100428, 4691.6201204358968, 2457.3031913645382, 484.6381729685242, 4835.6876329345241, 4767.2354501652117, 2282.0950066955152, 4898.4609196541051, 4672.335252695786, 4221.3308667034516, 4417.6626219480631, 3083.9058023984899, 4868.7157709201929, 786.07841306500393, 4944.2680363984327, 4937.7993211764588, 4803.3386317210561, 4879.2807663763724, 4797.9822980317922, 4852.9820592033821, 4899.6510533307228, 4928.9608925778584, 1246.8335875099469, 897.66721018295698, 4898.2341374426669, 4937.0728444214092, 4872.1592141034453, 3831.5949021985152, 1566.6161421109789, 3936.3140901846264, 4892.7020570555142, 4903.5393850364499, 3525.1357019198267, 4897.3182694623665, 5005.8531399460653, 3254.3045422728815, 4884.9698515829059, 4899.1494564183076, 4836.8030812818424, 4856.2122026589168, 4869.3180903740213, 547.36918112632191, 4846.7131244835628, 4982.6019861210279, 747.61377660510209, 4554.7305314356299, 4892.015973083784, 4912.1696179876153, 4898.0014011113362, 2264.5159469938517, 4960.50302352572, 4880.7201694811938, 4958.0815976196673, 2430.4497529729097, 4599.4426177979431, 4918.7410758634605, 4934.7694918568259, 4874.2873088301612, 2950.3373502059007, 4765.6751972876291, 4825.4326879605924, 4865.216482055388, 4867.8580832872149, 4846.6858676226166, 4797.9410234736597, 1604.1672705788772, 4883.545030150196, 4934.742667723307, 4834.6220107203899, 4880.485455310306, 4872.7146720968321, 4887.0968698907473, 4876.9333199175926, 4559.4601818769524, 2640.2274499064265, 1096.0929729729335, 4947.7991976891608, 4850.2130545872315, 4900.9058606835206, 4893.4886548532659, 4866.087965210766, 4811.0554404073273, 717.29699532908421, 4871.5388196706599, 4927.7429601568074, 4805.8047862866842, 4890.920569779707, 767.03434335074769]\n",
      "mean return 4103.93716085\n",
      "std of return 1360.52018715\n"
     ]
    }
   ],
   "source": [
    "returns = []  # the returned reward\n",
    "observations = []\n",
    "actions = []\n",
    "for i in range(num_rollouts):\n",
    "    print('iter', i)\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    totalr = 0.\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action = model.predict(obs[None,:])  # reshape it as (1, 376)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        totalr += r\n",
    "        steps += 1\n",
    "        if False:  # render it or not\n",
    "            env.render()\n",
    "        if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "    returns.append(totalr)\n",
    "\n",
    "print('returns', returns)\n",
    "print('mean return', np.mean(returns))\n",
    "print('std of return', np.std(returns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of reward\n",
    "- mean return 4103.93716085\n",
    "- std of return 1360.52018715"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAgger\n",
    "\n",
    "Get the new data from Expert policy by providing the new observations from our NN policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Volumes/JetDrive/MOOC/UCBerkeley/Reinforcement Learning 2017 Fall/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 20:28:58,742] From /Volumes/JetDrive/MOOC/UCBerkeley/Reinforcement Learning 2017 Fall/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Volumes/JetDrive/MOOC/UCBerkeley/Reinforcement Learning 2017 Fall/homework/hw1/tf_util.py:92: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 20:28:58,744] From /Volumes/JetDrive/MOOC/UCBerkeley/Reinforcement Learning 2017 Fall/homework/hw1/tf_util.py:92: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "iter 1\n",
      "iter 2\n",
      "iter 3\n",
      "iter 4\n",
      "iter 5\n",
      "iter 6\n",
      "iter 7\n",
      "iter 8\n",
      "iter 9\n",
      "iter 10\n",
      "iter 11\n",
      "iter 12\n",
      "iter 13\n",
      "iter 14\n",
      "iter 15\n",
      "iter 16\n",
      "iter 17\n",
      "iter 18\n",
      "iter 19\n",
      "iter 20\n",
      "iter 21\n",
      "iter 22\n",
      "iter 23\n",
      "iter 24\n",
      "iter 25\n",
      "iter 26\n",
      "iter 27\n",
      "iter 28\n",
      "iter 29\n",
      "iter 30\n",
      "iter 31\n",
      "iter 32\n",
      "iter 33\n",
      "iter 34\n",
      "iter 35\n",
      "iter 36\n",
      "iter 37\n",
      "iter 38\n",
      "iter 39\n",
      "iter 40\n",
      "iter 41\n",
      "iter 42\n",
      "iter 43\n",
      "iter 44\n",
      "iter 45\n",
      "iter 46\n",
      "iter 47\n",
      "iter 48\n",
      "iter 49\n",
      "iter 50\n",
      "iter 51\n",
      "iter 52\n",
      "iter 53\n",
      "iter 54\n",
      "iter 55\n",
      "iter 56\n",
      "iter 57\n",
      "iter 58\n",
      "iter 59\n",
      "iter 60\n",
      "iter 61\n",
      "iter 62\n",
      "iter 63\n",
      "iter 64\n",
      "iter 65\n",
      "iter 66\n",
      "iter 67\n",
      "iter 68\n",
      "iter 69\n",
      "iter 70\n",
      "iter 71\n",
      "iter 72\n",
      "iter 73\n",
      "iter 74\n",
      "iter 75\n",
      "iter 76\n",
      "iter 77\n",
      "iter 78\n",
      "iter 79\n",
      "iter 80\n",
      "iter 81\n",
      "iter 82\n",
      "iter 83\n",
      "iter 84\n",
      "iter 85\n",
      "iter 86\n",
      "iter 87\n",
      "iter 88\n",
      "iter 89\n",
      "iter 90\n",
      "iter 91\n",
      "iter 92\n",
      "iter 93\n",
      "iter 94\n",
      "iter 95\n",
      "iter 96\n",
      "iter 97\n",
      "iter 98\n",
      "iter 99\n"
     ]
    }
   ],
   "source": [
    "with tf.Session():\n",
    "    tf_util.initialize()\n",
    "    DAgger_num_rollouts = 100\n",
    "    DAgger_observations = []\n",
    "    DAgger_actions = []\n",
    "    for i in range(DAgger_num_rollouts):\n",
    "        print('iter', i)\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        totalr = 0.\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = model.predict(obs[None,:])  # Our policy action\n",
    "            expert_action = policy_fn(obs[None,:])  # expert policy action\n",
    "            DAgger_observations.append(obs)\n",
    "            DAgger_actions.append(expert_action)\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            totalr += r\n",
    "            steps += 1\n",
    "            if False:  # render it or not\n",
    "                env.render()\n",
    "            if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "        returns.append(totalr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAgger_obs = np.array(DAgger_observations)\n",
    "DAgger_act = np.array(DAgger_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2910, 376)\n",
      "(2910, 1, 17)\n"
     ]
    }
   ],
   "source": [
    "print(DAgger_obs.shape)\n",
    "print(DAgger_act.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_val = int(len(DAgger_obs) * .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = DAgger_obs[:split_val]\n",
    "y_train = DAgger_act[:split_val]\n",
    "x_val = DAgger_obs[split_val:]\n",
    "y_val = DAgger_act[split_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2328, 1, 17)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0], y_train.shape[2])\n",
    "y_val = y_val.reshape(y_val.shape[0], y_val.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2328, 376)\n",
      "(2328, 17)\n",
      "(582, 376)\n",
      "(582, 17)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct the callback\n",
    "filepath=\"data/best_epoch_DAgger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 256)               96512     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 17)                4369      \n",
      "=================================================================\n",
      "Total params: 232,465\n",
      "Trainable params: 232,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2328 samples, validate on 582 samples\n",
      "Epoch 1/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.3847Epoch 00000: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.3821 - val_loss: 0.1827\n",
      "Epoch 2/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.1390Epoch 00001: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.1387 - val_loss: 0.1194\n",
      "Epoch 3/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0981Epoch 00002: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0980 - val_loss: 0.0927\n",
      "Epoch 4/200\n",
      "2176/2328 [===========================>..] - ETA: 0s - loss: 0.0817Epoch 00003: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0805 - val_loss: 0.0825\n",
      "Epoch 5/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0686Epoch 00004: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0686 - val_loss: 0.0714\n",
      "Epoch 6/200\n",
      "2176/2328 [===========================>..] - ETA: 0s - loss: 0.0605Epoch 00005: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0601 - val_loss: 0.0736\n",
      "Epoch 7/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0553Epoch 00006: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0552 - val_loss: 0.0631\n",
      "Epoch 8/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0480Epoch 00007: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0486 - val_loss: 0.0557\n",
      "Epoch 9/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0443Epoch 00008: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0437 - val_loss: 0.0564\n",
      "Epoch 10/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0405Epoch 00009: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0410 - val_loss: 0.0564\n",
      "Epoch 11/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0380Epoch 00010: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0382 - val_loss: 0.0520\n",
      "Epoch 12/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0364Epoch 00011: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0362 - val_loss: 0.0507\n",
      "Epoch 13/200\n",
      "1984/2328 [========================>.....] - ETA: 0s - loss: 0.0340Epoch 00012: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0344 - val_loss: 0.0503\n",
      "Epoch 14/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0333Epoch 00013: val_loss improved from 0.04742 to 0.04619, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0330 - val_loss: 0.0462\n",
      "Epoch 15/200\n",
      "1984/2328 [========================>.....] - ETA: 0s - loss: 0.0301Epoch 00014: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0304 - val_loss: 0.0505\n",
      "Epoch 16/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0305Epoch 00015: val_loss improved from 0.04619 to 0.04435, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0303 - val_loss: 0.0444\n",
      "Epoch 17/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0261Epoch 00016: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0263 - val_loss: 0.0456\n",
      "Epoch 18/200\n",
      "2112/2328 [==========================>...] - ETA: 0s - loss: 0.0249Epoch 00017: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0248 - val_loss: 0.0447\n",
      "Epoch 19/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0235Epoch 00018: val_loss improved from 0.04435 to 0.04110, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0237 - val_loss: 0.0411\n",
      "Epoch 20/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0234Epoch 00019: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0238 - val_loss: 0.0417\n",
      "Epoch 21/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0218Epoch 00020: val_loss improved from 0.04110 to 0.04025, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0217 - val_loss: 0.0402\n",
      "Epoch 22/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0216Epoch 00021: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0216 - val_loss: 0.0416\n",
      "Epoch 23/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0238Epoch 00022: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0237 - val_loss: 0.0422\n",
      "Epoch 24/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0207Epoch 00023: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0205 - val_loss: 0.0404\n",
      "Epoch 25/200\n",
      "1728/2328 [=====================>........] - ETA: 0s - loss: 0.0204Epoch 00024: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0200 - val_loss: 0.0410\n",
      "Epoch 26/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0190Epoch 00025: val_loss improved from 0.04025 to 0.04018, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0189 - val_loss: 0.0402\n",
      "Epoch 27/200\n",
      "2176/2328 [===========================>..] - ETA: 0s - loss: 0.0200Epoch 00026: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0202 - val_loss: 0.0414\n",
      "Epoch 28/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0186Epoch 00027: val_loss improved from 0.04018 to 0.03853, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0186 - val_loss: 0.0385\n",
      "Epoch 29/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0179Epoch 00028: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0179 - val_loss: 0.0406\n",
      "Epoch 30/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0195Epoch 00029: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0195 - val_loss: 0.0389\n",
      "Epoch 31/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0188Epoch 00030: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0193 - val_loss: 0.0413\n",
      "Epoch 32/200\n",
      "1984/2328 [========================>.....] - ETA: 0s - loss: 0.0187Epoch 00031: val_loss improved from 0.03853 to 0.03811, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0184 - val_loss: 0.0381\n",
      "Epoch 33/200\n",
      "1984/2328 [========================>.....] - ETA: 0s - loss: 0.0179Epoch 00032: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0184 - val_loss: 0.0395\n",
      "Epoch 34/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0206Epoch 00033: val_loss improved from 0.03811 to 0.03772, saving model to data/best_epoch_DAgger.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2328/2328 [==============================] - 0s - loss: 0.0206 - val_loss: 0.0377\n",
      "Epoch 35/200\n",
      "1984/2328 [========================>.....] - ETA: 0s - loss: 0.0198Epoch 00034: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0195 - val_loss: 0.0381\n",
      "Epoch 36/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0167Epoch 00035: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0172 - val_loss: 0.0379\n",
      "Epoch 37/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0192Epoch 00036: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0191 - val_loss: 0.0518\n",
      "Epoch 38/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0174Epoch 00037: val_loss improved from 0.03772 to 0.03647, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0173 - val_loss: 0.0365\n",
      "Epoch 39/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0147Epoch 00038: val_loss improved from 0.03647 to 0.03413, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0147 - val_loss: 0.0341\n",
      "Epoch 40/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0141Epoch 00039: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0143 - val_loss: 0.0374\n",
      "Epoch 41/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0146Epoch 00040: val_loss improved from 0.03413 to 0.03402, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0146 - val_loss: 0.0340\n",
      "Epoch 42/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0141Epoch 00041: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0138 - val_loss: 0.0366\n",
      "Epoch 43/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0129Epoch 00042: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0129 - val_loss: 0.0360\n",
      "Epoch 44/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0142Epoch 00043: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0144 - val_loss: 0.0371\n",
      "Epoch 45/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0130Epoch 00044: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0134 - val_loss: 0.0369\n",
      "Epoch 46/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0157Epoch 00045: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0153 - val_loss: 0.0378\n",
      "Epoch 47/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0142Epoch 00046: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0140 - val_loss: 0.0376\n",
      "Epoch 48/200\n",
      "1984/2328 [========================>.....] - ETA: 0s - loss: 0.0133Epoch 00047: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0136 - val_loss: 0.0350\n",
      "Epoch 49/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0135Epoch 00048: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0135 - val_loss: 0.0363\n",
      "Epoch 50/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0133Epoch 00049: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0134 - val_loss: 0.0347\n",
      "Epoch 51/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0120Epoch 00050: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0121 - val_loss: 0.0349\n",
      "Epoch 52/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0124Epoch 00051: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0124 - val_loss: 0.0360\n",
      "Epoch 53/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0116Epoch 00052: val_loss improved from 0.03402 to 0.03340, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0117 - val_loss: 0.0334\n",
      "Epoch 54/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0111Epoch 00053: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0112 - val_loss: 0.0343\n",
      "Epoch 55/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0115Epoch 00054: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0115 - val_loss: 0.0341\n",
      "Epoch 56/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0109Epoch 00055: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0109 - val_loss: 0.0338\n",
      "Epoch 57/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0110Epoch 00056: val_loss improved from 0.03340 to 0.03306, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0110 - val_loss: 0.0331\n",
      "Epoch 58/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0127Epoch 00057: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0127 - val_loss: 0.0385\n",
      "Epoch 59/200\n",
      "2176/2328 [===========================>..] - ETA: 0s - loss: 0.0125Epoch 00058: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0125 - val_loss: 0.0355\n",
      "Epoch 60/200\n",
      "1984/2328 [========================>.....] - ETA: 0s - loss: 0.0137Epoch 00059: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0135 - val_loss: 0.0404\n",
      "Epoch 61/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0120Epoch 00060: val_loss improved from 0.03306 to 0.03216, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0120 - val_loss: 0.0322\n",
      "Epoch 62/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0106Epoch 00061: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0106 - val_loss: 0.0332\n",
      "Epoch 63/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0101Epoch 00062: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0103 - val_loss: 0.0322\n",
      "Epoch 64/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0110Epoch 00063: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0109 - val_loss: 0.0340\n",
      "Epoch 65/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0111Epoch 00064: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0110 - val_loss: 0.0336\n",
      "Epoch 66/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0116Epoch 00065: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0116 - val_loss: 0.0343\n",
      "Epoch 67/200\n",
      "2112/2328 [==========================>...] - ETA: 0s - loss: 0.0111Epoch 00066: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0111 - val_loss: 0.0339\n",
      "Epoch 68/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0133Epoch 00067: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0127 - val_loss: 0.0342\n",
      "Epoch 69/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0103Epoch 00068: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0105 - val_loss: 0.0330\n",
      "Epoch 70/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0113Epoch 00069: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0113 - val_loss: 0.0344\n",
      "Epoch 71/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0104Epoch 00070: val_loss improved from 0.03216 to 0.03199, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0105 - val_loss: 0.0320\n",
      "Epoch 72/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0108Epoch 00071: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0110 - val_loss: 0.0338\n",
      "Epoch 73/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0122Epoch 00072: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0119 - val_loss: 0.0345\n",
      "Epoch 74/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0121Epoch 00073: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0120 - val_loss: 0.0336\n",
      "Epoch 75/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0111Epoch 00074: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0111 - val_loss: 0.0336\n",
      "Epoch 76/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0101Epoch 00075: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0104 - val_loss: 0.0341\n",
      "Epoch 77/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0100Epoch 00076: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0101 - val_loss: 0.0331\n",
      "Epoch 78/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0097Epoch 00077: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0099 - val_loss: 0.0327\n",
      "Epoch 79/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0094Epoch 00078: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0095 - val_loss: 0.0323\n",
      "Epoch 80/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0088Epoch 00079: val_loss improved from 0.03199 to 0.03128, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0091 - val_loss: 0.0313\n",
      "Epoch 81/200\n",
      "2112/2328 [==========================>...] - ETA: 0s - loss: 0.0087Epoch 00080: val_loss improved from 0.03128 to 0.03058, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0089 - val_loss: 0.0306\n",
      "Epoch 82/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0097Epoch 00081: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0097 - val_loss: 0.0338\n",
      "Epoch 83/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0100Epoch 00082: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0099 - val_loss: 0.0323\n",
      "Epoch 84/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0094Epoch 00083: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0093 - val_loss: 0.0320\n",
      "Epoch 85/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0084Epoch 00084: val_loss improved from 0.03058 to 0.03014, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0085 - val_loss: 0.0301\n",
      "Epoch 86/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00085: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0092 - val_loss: 0.0319\n",
      "Epoch 87/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0091Epoch 00086: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0091 - val_loss: 0.0322\n",
      "Epoch 88/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0089Epoch 00087: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0089 - val_loss: 0.0320\n",
      "Epoch 89/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0096Epoch 00088: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0096 - val_loss: 0.0318\n",
      "Epoch 90/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0095Epoch 00089: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0094 - val_loss: 0.0303\n",
      "Epoch 91/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0111Epoch 00090: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0110 - val_loss: 0.0314\n",
      "Epoch 92/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0109Epoch 00091: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0110 - val_loss: 0.0358\n",
      "Epoch 93/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0110Epoch 00092: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0108 - val_loss: 0.0312\n",
      "Epoch 94/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00093: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0092 - val_loss: 0.0306\n",
      "Epoch 95/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0093Epoch 00094: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0092 - val_loss: 0.0319\n",
      "Epoch 96/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0092Epoch 00095: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0093 - val_loss: 0.0332\n",
      "Epoch 97/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0097Epoch 00096: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0097 - val_loss: 0.0338\n",
      "Epoch 98/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0102Epoch 00097: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0102 - val_loss: 0.0316\n",
      "Epoch 99/200\n",
      "2112/2328 [==========================>...] - ETA: 0s - loss: 0.0090Epoch 00098: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0092 - val_loss: 0.0315\n",
      "Epoch 100/200\n",
      "2112/2328 [==========================>...] - ETA: 0s - loss: 0.0097Epoch 00099: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0096 - val_loss: 0.0320\n",
      "Epoch 101/200\n",
      "2176/2328 [===========================>..] - ETA: 0s - loss: 0.0095Epoch 00100: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0097 - val_loss: 0.0342\n",
      "Epoch 102/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0098Epoch 00101: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0098 - val_loss: 0.0309\n",
      "Epoch 103/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0087Epoch 00102: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0086 - val_loss: 0.0310\n",
      "Epoch 104/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0086Epoch 00103: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0086 - val_loss: 0.0305\n",
      "Epoch 105/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0084Epoch 00104: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0307\n",
      "Epoch 106/200\n",
      "2112/2328 [==========================>...] - ETA: 0s - loss: 0.0081Epoch 00105: val_loss improved from 0.03014 to 0.02897, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0080 - val_loss: 0.0290\n",
      "Epoch 107/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0085Epoch 00106: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0085 - val_loss: 0.0360\n",
      "Epoch 108/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0085Epoch 00107: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0315\n",
      "Epoch 109/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0089Epoch 00108: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0090 - val_loss: 0.0298\n",
      "Epoch 110/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0094Epoch 00109: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0094 - val_loss: 0.0327\n",
      "Epoch 111/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0092Epoch 00110: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0093 - val_loss: 0.0329\n",
      "Epoch 112/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0098Epoch 00111: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2328/2328 [==============================] - 0s - loss: 0.0095 - val_loss: 0.0304\n",
      "Epoch 113/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0083Epoch 00112: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0082 - val_loss: 0.0306\n",
      "Epoch 114/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0080Epoch 00113: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0080 - val_loss: 0.0303\n",
      "Epoch 115/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0075Epoch 00114: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0074 - val_loss: 0.0300\n",
      "Epoch 116/200\n",
      "2176/2328 [===========================>..] - ETA: 0s - loss: 0.0087Epoch 00115: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0087 - val_loss: 0.0339\n",
      "Epoch 117/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0116Epoch 00116: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0116 - val_loss: 0.0392\n",
      "Epoch 118/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0215Epoch 00117: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0215 - val_loss: 0.0383\n",
      "Epoch 119/200\n",
      "2176/2328 [===========================>..] - ETA: 0s - loss: 0.0344Epoch 00118: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0353 - val_loss: 0.0606\n",
      "Epoch 120/200\n",
      "1984/2328 [========================>.....] - ETA: 0s - loss: 0.0334Epoch 00119: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0325 - val_loss: 0.0442\n",
      "Epoch 121/200\n",
      "2176/2328 [===========================>..] - ETA: 0s - loss: 0.0245Epoch 00120: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0244 - val_loss: 0.0384\n",
      "Epoch 122/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0189Epoch 00121: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0190 - val_loss: 0.0435\n",
      "Epoch 123/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0194Epoch 00122: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0194 - val_loss: 0.0372\n",
      "Epoch 124/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0151Epoch 00123: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0150 - val_loss: 0.0307\n",
      "Epoch 125/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0138Epoch 00124: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0138 - val_loss: 0.0362\n",
      "Epoch 126/200\n",
      "2112/2328 [==========================>...] - ETA: 0s - loss: 0.0142Epoch 00125: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0146 - val_loss: 0.0321\n",
      "Epoch 127/200\n",
      "1984/2328 [========================>.....] - ETA: 0s - loss: 0.0130Epoch 00126: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0126 - val_loss: 0.0317\n",
      "Epoch 128/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0108Epoch 00127: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0107 - val_loss: 0.0297\n",
      "Epoch 129/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0096Epoch 00128: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0096 - val_loss: 0.0295\n",
      "Epoch 130/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0090Epoch 00129: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0090 - val_loss: 0.0310\n",
      "Epoch 131/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0085Epoch 00130: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0294\n",
      "Epoch 132/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0084Epoch 00131: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0308\n",
      "Epoch 133/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0084Epoch 00132: val_loss improved from 0.02897 to 0.02895, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0083 - val_loss: 0.0290\n",
      "Epoch 134/200\n",
      "2176/2328 [===========================>..] - ETA: 0s - loss: 0.0078Epoch 00133: val_loss improved from 0.02895 to 0.02798, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0280\n",
      "Epoch 135/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0079Epoch 00134: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0290\n",
      "Epoch 136/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0081Epoch 00135: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0081 - val_loss: 0.0309\n",
      "Epoch 137/200\n",
      "2176/2328 [===========================>..] - ETA: 0s - loss: 0.0077Epoch 00136: val_loss improved from 0.02798 to 0.02793, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0279\n",
      "Epoch 138/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00137: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0293\n",
      "Epoch 139/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00138: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0082 - val_loss: 0.0285\n",
      "Epoch 140/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0088Epoch 00139: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0088 - val_loss: 0.0298\n",
      "Epoch 141/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0092Epoch 00140: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0092 - val_loss: 0.0288\n",
      "Epoch 142/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0082Epoch 00141: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0083 - val_loss: 0.0295\n",
      "Epoch 143/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00142: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0077 - val_loss: 0.0295\n",
      "Epoch 144/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00143: val_loss improved from 0.02793 to 0.02777, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0075 - val_loss: 0.0278\n",
      "Epoch 145/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00144: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0283\n",
      "Epoch 146/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00145: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0283\n",
      "Epoch 147/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00146: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0082 - val_loss: 0.0300\n",
      "Epoch 148/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0084Epoch 00147: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0288\n",
      "Epoch 149/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00148: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0074 - val_loss: 0.0280\n",
      "Epoch 150/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0075Epoch 00149: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0075 - val_loss: 0.0292\n",
      "Epoch 151/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0076Epoch 00150: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0280\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0075Epoch 00151: val_loss improved from 0.02777 to 0.02773, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0277\n",
      "Epoch 153/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0085Epoch 00152: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0085 - val_loss: 0.0287\n",
      "Epoch 154/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0086Epoch 00153: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0292\n",
      "Epoch 155/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0081Epoch 00154: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0080 - val_loss: 0.0297\n",
      "Epoch 156/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0083Epoch 00155: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0280\n",
      "Epoch 157/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0075Epoch 00156: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0280\n",
      "Epoch 158/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0078Epoch 00157: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0287\n",
      "Epoch 159/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0076Epoch 00158: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0078 - val_loss: 0.0283\n",
      "Epoch 160/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0075Epoch 00159: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0284\n",
      "Epoch 161/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0076Epoch 00160: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0075 - val_loss: 0.0288\n",
      "Epoch 162/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0075Epoch 00161: val_loss improved from 0.02773 to 0.02750, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0275\n",
      "Epoch 163/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0080Epoch 00162: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0080 - val_loss: 0.0299\n",
      "Epoch 164/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0076Epoch 00163: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0286\n",
      "Epoch 165/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00164: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0073 - val_loss: 0.0278\n",
      "Epoch 166/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0079Epoch 00165: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0287\n",
      "Epoch 167/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0078Epoch 00166: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0078 - val_loss: 0.0292\n",
      "Epoch 168/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0078Epoch 00167: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0078 - val_loss: 0.0287\n",
      "Epoch 169/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0084Epoch 00168: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0286\n",
      "Epoch 170/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00169: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0082 - val_loss: 0.0300\n",
      "Epoch 171/200\n",
      "1792/2328 [======================>.......] - ETA: 0s - loss: 0.0084Epoch 00170: val_loss improved from 0.02750 to 0.02709, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0082 - val_loss: 0.0271\n",
      "Epoch 172/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00171: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0077 - val_loss: 0.0293\n",
      "Epoch 173/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0080Epoch 00172: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0081 - val_loss: 0.0302\n",
      "Epoch 174/200\n",
      "1856/2328 [======================>.......] - ETA: 0s - loss: 0.0106Epoch 00173: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0106 - val_loss: 0.0301\n",
      "Epoch 175/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0091Epoch 00174: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0090 - val_loss: 0.0300\n",
      "Epoch 176/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0077Epoch 00175: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0077 - val_loss: 0.0277\n",
      "Epoch 177/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0073Epoch 00176: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0073 - val_loss: 0.0279\n",
      "Epoch 178/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0071Epoch 00177: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0070 - val_loss: 0.0292\n",
      "Epoch 179/200\n",
      "1984/2328 [========================>.....] - ETA: 0s - loss: 0.0067Epoch 00178: val_loss improved from 0.02709 to 0.02709, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0069 - val_loss: 0.0271\n",
      "Epoch 180/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00179: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0070 - val_loss: 0.0280\n",
      "Epoch 181/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0070Epoch 00180: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0070 - val_loss: 0.0279\n",
      "Epoch 182/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0067Epoch 00181: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0067 - val_loss: 0.0273\n",
      "Epoch 183/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0067Epoch 00182: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0067 - val_loss: 0.0281\n",
      "Epoch 184/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0068Epoch 00183: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0068 - val_loss: 0.0273\n",
      "Epoch 185/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0073Epoch 00184: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0073 - val_loss: 0.0278\n",
      "Epoch 186/200\n",
      "1984/2328 [========================>.....] - ETA: 0s - loss: 0.0074Epoch 00185: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0311\n",
      "Epoch 187/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0093Epoch 00186: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0091 - val_loss: 0.0286\n",
      "Epoch 188/200\n",
      "2112/2328 [==========================>...] - ETA: 0s - loss: 0.0083Epoch 00187: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0082 - val_loss: 0.0308\n",
      "Epoch 189/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0084Epoch 00188: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0084 - val_loss: 0.0283\n",
      "Epoch 190/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0075Epoch 00189: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0076 - val_loss: 0.0282\n",
      "Epoch 191/200\n",
      "2112/2328 [==========================>...] - ETA: 0s - loss: 0.0068Epoch 00190: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2328/2328 [==============================] - 0s - loss: 0.0067 - val_loss: 0.0274\n",
      "Epoch 192/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0065Epoch 00191: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0065 - val_loss: 0.0273\n",
      "Epoch 193/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0069Epoch 00192: val_loss improved from 0.02709 to 0.02669, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0069 - val_loss: 0.0267\n",
      "Epoch 194/200\n",
      "2240/2328 [===========================>..] - ETA: 0s - loss: 0.0091Epoch 00193: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0093 - val_loss: 0.0341\n",
      "Epoch 195/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0105Epoch 00194: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0103 - val_loss: 0.0332\n",
      "Epoch 196/200\n",
      "2112/2328 [==========================>...] - ETA: 0s - loss: 0.0085Epoch 00195: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0083 - val_loss: 0.0279\n",
      "Epoch 197/200\n",
      "2048/2328 [=========================>....] - ETA: 0s - loss: 0.0072Epoch 00196: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0073 - val_loss: 0.0280\n",
      "Epoch 198/200\n",
      "1920/2328 [=======================>......] - ETA: 0s - loss: 0.0070Epoch 00197: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0069 - val_loss: 0.0297\n",
      "Epoch 199/200\n",
      "2176/2328 [===========================>..] - ETA: 0s - loss: 0.0067Epoch 00198: val_loss improved from 0.02669 to 0.02629, saving model to data/best_epoch_DAgger.hdf5\n",
      "2328/2328 [==============================] - 0s - loss: 0.0068 - val_loss: 0.0263\n",
      "Epoch 200/200\n",
      "2304/2328 [============================>.] - ETA: 0s - loss: 0.0074Epoch 00199: val_loss did not improve\n",
      "2328/2328 [==============================] - 0s - loss: 0.0074 - val_loss: 0.0292\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ede57f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Start Training with new DAgger Data\n",
    "model.summary()\n",
    "history_w_model = model.fit(x_train, y_train, callbacks=callbacks_list, epochs=num_epochs, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "plt.plot(history_w_model.history['loss'], label='loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.plot(history_w_model.history['val_loss'], label='Val_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load weights into the model\n",
    "model.load_weights(\"data/best_epoch_DAgger.hdf5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 1\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 2\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 3\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 4\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 5\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 6\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 7\n",
      "100/500\n",
      "iter 8\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 9\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 10\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 11\n",
      "100/500\n",
      "200/500\n",
      "iter 12\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 13\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 14\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 15\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 16\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 17\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 18\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 19\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 20\n",
      "100/500\n",
      "iter 21\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 22\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 23\n",
      "100/500\n",
      "iter 24\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 25\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 26\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 27\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 28\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 29\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 30\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 31\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 32\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 33\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 34\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 35\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 36\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 37\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 38\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 39\n",
      "100/500\n",
      "iter 40\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 41\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 42\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 43\n",
      "100/500\n",
      "200/500\n",
      "iter 44\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 45\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 46\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 47\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 48\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 49\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 50\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 51\n",
      "100/500\n",
      "iter 52\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 53\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 54\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 55\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 56\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 57\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 58\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 59\n",
      "100/500\n",
      "iter 60\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 61\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 62\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 63\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 64\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 65\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 66\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 67\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 68\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 69\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 70\n",
      "100/500\n",
      "200/500\n",
      "iter 71\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 72\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 73\n",
      "100/500\n",
      "iter 74\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "iter 75\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 76\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 77\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 78\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 79\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 80\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 81\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 82\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 83\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 84\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 85\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 86\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 87\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 88\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 89\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 90\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "iter 91\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 92\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 93\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 94\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 95\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 96\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 97\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 98\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "iter 99\n",
      "100/500\n",
      "200/500\n",
      "300/500\n",
      "400/500\n",
      "500/500\n",
      "returns [2883.9568544504168, 4859.1175378092257, 3150.3618614253796, 4906.5446241406753, 4908.0963343434678, 4809.3533550862294, 4855.8780774629558, 1408.2014398515166, 3621.6919841998069, 4864.1233471257328, 4872.7868827015154, 2442.6130974627004, 4889.1700236783454, 4907.3145615552021, 4843.8138559696126, 4880.7569854312915, 4945.0319583563514, 4870.4355539905364, 4854.4183953888869, 4924.7847782104591, 1553.6758068568251, 4892.5160183588105, 4794.948705041118, 1392.0548258427718, 4878.2011233701287, 4879.3029790837909, 4886.4407875680981, 4842.1485161058163, 3724.943819525155, 4884.8823910346218, 4864.3196117189045, 4810.7059155002498, 3261.426548255838, 4939.6376779948487, 4933.5001583930325, 4904.8136638844571, 4845.6477312346096, 4902.3688557505548, 4844.8572748072538, 702.4933612284774, 4871.9280498350008, 4856.6297753831905, 4887.7922770925925, 2586.1603335232599, 3448.0706551898938, 4896.1956543702845, 2942.5738727751277, 4818.7188897679907, 4197.9530753295257, 4818.8340433142521, 4582.027887968311, 1124.0834569994288, 4864.9492519394771, 4048.3323764657516, 3479.0693477840587, 4843.4530885838758, 4848.384059967635, 4806.0581385980959, 2820.6815018355492, 1457.5238701875194, 4913.509072974789, 4855.818314504656, 4938.7012818234307, 4815.2788110950632, 4245.3183980386402, 3164.3645819108747, 4862.6715215668373, 3564.2398223273431, 4884.8829685482151, 4689.0023419205299, 1619.5551850342083, 4915.0498269409536, 4932.4645429951561, 826.53353419104633, 3447.4735338940018, 4887.1670634895299, 4844.6676456751447, 4881.6561589112862, 4913.8331098260705, 4829.146782151839, 4891.1114548753139, 4946.2133024359646, 4854.1973730362652, 4821.3006281697317, 4134.3170297548813, 4824.2117092108447, 4859.2386984212753, 4883.1963429988673, 4903.9614604276176, 4918.2423863479189, 4483.411013091265, 4877.0614628528801, 4706.29228307639, 4869.5846161449308, 4871.5768866890257, 4784.1188388091878, 4863.4391037799533, 4959.8696026831676, 4898.1060346531203, 4819.4190896018408]\n",
      "mean return 4308.36960702\n",
      "std of return 1085.95059494\n"
     ]
    }
   ],
   "source": [
    "returns = []  # the returned reward\n",
    "observations = []\n",
    "actions = []\n",
    "for i in range(num_rollouts):\n",
    "    print('iter', i)\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    totalr = 0.\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action = model.predict(obs[None,:])  # reshape it as (1, 376)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        totalr += r\n",
    "        steps += 1\n",
    "        if False:  # render it or not\n",
    "            env.render()\n",
    "        if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "    returns.append(totalr)\n",
    "\n",
    "print('returns', returns)\n",
    "print('mean return', np.mean(returns))\n",
    "print('std of return', np.std(returns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The result of reward\n",
    "- mean return 4308.36960702\n",
    "- std of return 1085.95059494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
